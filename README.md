# CE493 Project: Eyeriss-Style CNN Accelerator

## Description

This project is an implementation of a single convolutional layer accelerator, inspired by the Eyeriss architecture, for CE493.

The dataset we elected to use for testing was MNIST. We followed a complete hardware verification flow, starting from high-level software models down to hardware simulation:

1. **Floating-Point Model:** First, we trained a standard floating-point CNN in PyTorch on the MNIST dataset (`model_fp.py`). The purpose of this was to generate a realistic, trained set of weights (`mnist_cnn.pth`).

2. **Bit-Accurate Golden Model:** We then developed a script to convert the floating point weights to fixed point (`model_q.py`), as well as a golden reference model using those weights (`model_golden.py`). This model loads the quantized weights and emulates the exact bit-accurate hardware datapath to produce a "perfect" output file.

3. **RTL & Testbench Development:** Finally, we designed the accelerator RTL in Verilog. We also developed a Verilog testbench that reads the stimulus files (inputs and weights) and the golden_output.hex file generated by our Python scripts. This testbench feeds the RTL and automatically compares its output, bit-for-bit, against the golden model to verify hardware correctness.

-----

## Verification Flow

This project's verification flow is sequential. Run the following commands in order from your terminal.

### 1\. Train the Floating-Point Model

```sh 
/sw > python model_fp.py
```
### 2\. Quantize Data
```sh
/sw > python model_q.py
```

### 3\. Run the Bit-Accurate Golden Model
```sh
/sw > python model_golden.py
```

### 4\. Generate RTL Stimulus Files

This final script converts all `.npy` data files into 16-bit 2's complement hexadecimal text files for Verilog.

```sh
/sw > python save_to_hex.py
```

### 5\. Run RTL Simulation

```sh
/sim/eyeriss_top_mnist_sim > vsim -do eyeriss_top_sim.do
```
-----

## Hardware Datapath Specification

The golden model and RTL are designed to match this fixed-point specification:

  * **Data Format:** 16-bit Fixed-Point (**Q2.14**)
      * 1 Sign Bit
      * 1 Integer Bit
      * 14 Fractional Bits
      * *Real-World Range:* `[-2.0 to +1.999...]`
  * **Accumulator Width:** 32-bit Signed
      * This prevents overflow during the multiply-accumulate (MAC) operations for a single output pixel.

-----

Here is the updated README markdown. I have added a detailed **RTL Architecture** section following your Hardware Datapath Specification. This breaks down the hierarchy from the top level down to the individual processing elements, explaining the dataflow and purpose of each module.

-----

# CE493 Project: Eyeriss-Style CNN Accelerator

## Description

This project is an implementation of a single convolutional layer accelerator, inspired by the Eyeriss architecture, for CE493.

The dataset we elected to use for testing was MNIST. We followed a complete hardware verification flow, starting from high-level software models down to hardware simulation:

1.  **Floating-Point Model:** First, we trained a standard floating-point CNN in PyTorch on the MNIST dataset (`model_fp.py`). The purpose of this was to generate a realistic, trained set of weights (`mnist_cnn.pth`).

2.  **Bit-Accurate Golden Model:** We then developed a script to convert the floating point weights to fixed point (`model_q.py`), as well as a golden reference model using those weights (`model_golden.py`). This model loads the quantized weights and emulates the exact bit-accurate hardware datapath to produce a "perfect" output file.

3.  **RTL & Testbench Development:** Finally, we designed the accelerator RTL in Verilog. We also developed a Verilog testbench that reads the stimulus files (inputs and weights) and the golden\_output.hex file generated by our Python scripts. This testbench feeds the RTL and automatically compares its output, bit-for-bit, against the golden model to verify hardware correctness.

-----

## Verification Flow

This project's verification flow is sequential. Run the following commands in order from your terminal.

### 1\. Train the Floating-Point Model

```sh
/sw > python model_fp.py
```

### 2\. Quantize Data

```sh
/sw > python model_q.py
```

### 3\. Run the Bit-Accurate Golden Model

```sh
/sw > python model_golden.py
```

### 4\. Generate RTL Stimulus Files

This final script converts all `.npy` data files into 16-bit 2's complement hexadecimal text files for Verilog.

```sh
/sw > python save_to_hex.py
```

### 5\. Run RTL Simulation

```sh
/sim/eyeriss_top_mnist_sim > vsim -do eyeriss_top_sim.do
```

-----

## Hardware Datapath Specification

The golden model and RTL are designed to match this fixed-point specification:

  * **Data Format:** 16-bit Fixed-Point (**Q2.14**)
      * 1 Sign Bit
      * 1 Integer Bit
      * 14 Fractional Bits
      * *Real-World Range:* `[-2.0 to +1.999...]`
  * **Accumulator Width:** 32-bit Signed
      * This prevents overflow during the multiply-accumulate (MAC) operations for a single output pixel.

-----

## RTL Architecture

The hardware design follows a hierarchical structure typical of systolic array accelerators. Below is a breakdown of the Verilog modules.

### 1\. Top Level & Memory Hierarchy

  * **`eyeriss_top.sv`**

      * The top-level wrapper module. It integrates the Global Buffer (`buf_array`) and the Network-on-Chip (`noc`).
      * Exposes BRAM write ports for the testbench to load initial Weights and Input Feature Maps (Ifmaps).
      * Exposes FIFO read ports to stream out the calculated Partial Sums (Psums).

  * **`buf_array.sv` (Global Buffer Controller)**

      * Acts as the on-chip Global Buffer (GLB).
      * **Function:** Manages reading data from the static SRAMs (`buffer`) and pushing it into the execution pipeline via FIFOs.
      * **Logic:** Contains address generation logic to map the linear memory addresses of the image to the specific access patterns required by the array (handling striding and row mapping).
      * **Sub-modules:** Instantiates `buffer` (for storage) and `fifo` (for elastic buffering to the NoC).

  * **`buffer.sv`**

      * A standard inferable Block RAM (BRAM) used for storing the full Input Feature Map and Weights before processing begins.

### 2\. Network & Control

  * **`noc.sv` (Network-on-Chip / Scheduler)**

      * **Function:** Acts as the bridge between the Global Buffer and the Processing Element (PE) Array.
      * **Skewing Logic:** The primary responsibility of this module is to handle the **input skewing**. In a systolic array, inputs must be staggered so they arrive at the correct PE at the exact moment the accumulation wavefront passes through. The `noc` manages the `CALC_S` state to start specific rows at specific cycle counts.
      * **Output Management:** Collects the vertical Partial Sum streams from the array and buffers them into output FIFOs.

  * **`fifo.sv`**

      * A generic First-In-First-Out memory buffer used throughout the design (between the Buffer Array and NoC, and between NoC and output) to manage flow control and decouple memory access timing from compute timing.

### 3\. Compute Core

  * **`pe_array.sv`**

      * A 2D Systolic Array of Processing Elements.
      * **Dataflow:**
          * **Weights:** Broadcast horizontally (loaded into PEs and held stationary).
          * **Ifmaps:** Flow **Diagonally**. Input enters at `[c][r]` and passes to `[c+1][r-1]`. This reuse pattern is specific to the Eyeriss Row-Stationary style dataflow to maximize input reuse.
          * **Psums:** Flow **Vertically**. Partial sums are accumulated from Top to Bottom.

  * **`pe.sv` (Processing Element)**

      * The arithmetic unit of the accelerator.
      * **Components:**
          * **Weight Register:** Stores the kernel weight (Stationary).
          * **Ifmap Shift Register:** Pass-through logic for the diagonal dataflow.
          * **MAC Unit:** Performs `Psum = Psum_in + (Ifmap * Weight)`.
      * **FSM:**
        1.  `IDLE_S`: Waits for valid data.
        2.  `CALC_S`: Performs the multiplication and accumulation sequence for the kernel size.
        3.  `ACUM_S`: Adds the calculated chunk to the partial sum arriving from the PE above.
        4.  `WAIT_S`: Handshaking delay if necessary.

-----